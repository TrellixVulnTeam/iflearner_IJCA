{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import*\n",
    "data = datasets.load_wine()\n",
    "x_host=data.data[:,:8]\n",
    "x_guest=data.data[:,8:]\n",
    "feature_host=data.feature_names[:8]\n",
    "feature_guest=data.feature_names[8:]\n",
    "y=data.target\n",
    "x_host=pd.DataFrame(x_host,columns=feature_host)\n",
    "x_guest=pd.DataFrame(x_guest,columns=feature_guest)\n",
    "y=to_categorical(y,num_classes=3)\n",
    "y_true_pred=pd.DataFrame({\"label\":list(np.concatenate((y,np.zeros(np.shape(y))),axis=1))})\n",
    "data_all = pd.concat([x_host, y_true_pred],axis=1)\n",
    "build_tree_data=[data_all]\n",
    "build_tree_node=[\"root\"]\n",
    "tree_dict={}\n",
    "tree_branch_index=0\n",
    "current_depth=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 300\n",
    "# 学习率\n",
    "learning_rate = 0.001\n",
    "# 结点分裂最小样本数\n",
    "min_samples_split = 2\n",
    "# 结点最小基尼不纯度\n",
    "min_gini_impurity = 999\n",
    "max_depth = 2\n",
    "n_estimators = 300\n",
    "# 学习率\n",
    "learning_rate = 0.001\n",
    "# 结点分裂最小样本数\n",
    "min_samples_split =  2\n",
    "# 结点最小基尼不纯度\n",
    "min_gini_impurity = 999\n",
    "# 树最大深度\n",
    "max_depth =  2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.552713678800501e-15\n"
     ]
    }
   ],
   "source": [
    "# calucate_gini\n",
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "# 定义Logit损失\n",
    "\n",
    "\n",
    "class LogisticLoss:\n",
    "    def __init__(self):\n",
    "        sigmoid = Sigmoid()\n",
    "        self._func = sigmoid\n",
    "        self._grad = sigmoid.gradient\n",
    "\n",
    "    # 定义损失函数形式\n",
    "    def loss(self, y, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        p = self._func(y_pred)\n",
    "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
    "\n",
    "    # 定义一阶梯度\n",
    "    def gradient(self, y, y_pred):\n",
    "        p = self._func(y_pred)\n",
    "        return -(y - p)\n",
    "\n",
    "    # 定义二阶梯度\n",
    "    def hess(self, y, y_pred):\n",
    "        p = self._func(y_pred)\n",
    "        return p * (1 - p)\n",
    "\n",
    "\n",
    "def gain( y, y_pred, request=\"server\"):\n",
    "    loss=LogisticLoss()\n",
    "    if request == \"server\":\n",
    "        Gradient = np.power((y * loss.gradient(y, y_pred)).sum(), 2)\n",
    "        # Hessian矩阵计算\n",
    "        Hessian = loss.hess(y, y_pred).sum()\n",
    "        return 0.5 * (Gradient / Hessian)\n",
    "    elif request == \"client\":\n",
    "        return y * loss.gradient(y, y_pred), loss.hess(y, y_pred)\n",
    "def node_split( y):\n",
    "    # 中间特征所在列\n",
    "    feature = int(np.shape(y)[1]/2)\n",
    "    # 左子树为真实值，右子树为预测值\n",
    "    y_true, y_pred = y[:, :feature], y[:, feature:]\n",
    "    return y_true, y_pred\n",
    "def calculate_gini_impurity( y, y1, y2, request=\"server\"):\n",
    "    if request == \"server\":\n",
    "        y = np.array(y.to_list())\n",
    "        y1 = np.array(y1[\"label\"].to_list())\n",
    "        y2 = np.array(y2[\"label\"].to_list())\n",
    "        y_true, y_pred = node_split(y)\n",
    "        y1, y1_pred = node_split(y1)\n",
    "        y2, y2_pred = node_split(y2)\n",
    "        true_gain = gain(y1, y1_pred, )\n",
    "        false_gain = gain(y2, y2_pred, )\n",
    "        gain_1 = gain(y_true, y_pred, )\n",
    "        return true_gain + false_gain - gain_1\n",
    "    else:\n",
    "        y = np.array(y.to_list())\n",
    "        y1 = np.array(y1[\"label\"].to_list())\n",
    "        y2 = np.array(y2[\"label\"].to_list())\n",
    "        y_true, y_pred = node_split(y)\n",
    "        y1, y1_pred = node_split(y1)\n",
    "        y2, y2_pred = node_split(y2)\n",
    "        true_gain_gradient, true_gain_hess = gain(y1, y1_pred, request)\n",
    "        false_gain_gradient, false_gain_hess = gain(y2, y2_pred, request)\n",
    "        gain_gradient, gain_hess = gain(y_true, y_pred, request)\n",
    "        return true_gain_gradient, true_gain_hess, false_gain_gradient, false_gain_hess, gain_gradient, gain_hess\n",
    "\n",
    "\n",
    "def feature_split(X, feature_i, threshold):\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        def split_func(sample): return sample[feature_i] >= threshold\n",
    "    else:\n",
    "        def split_func(sample): return sample[feature_i] == threshold\n",
    "\n",
    "    X_left = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_right = np.array([sample for sample in X if not split_func(sample)])\n",
    "    return np.array([X_left, X_right])\n",
    "data = build_tree_data.pop(0)\n",
    "x=data.iloc[:,:-1]\n",
    "y_true_pred=data.iloc[:,-1]\n",
    "init_gini_impurity = 999\n",
    "# 合并输入和标签\n",
    "xy = pd.concat((x, y_true_pred), axis=1)\n",
    "# 获取样本数和特征数\n",
    "n_samples, n_features = x.shape\n",
    "if n_samples >= min_samples_split and current_depth <= max_depth:\n",
    "    # 遍历计算每个特征的基尼不纯度\n",
    "    feature_list = feature_host\n",
    "    for feature_i in range(n_features):\n",
    "        # 获取第i特征的所有取值\n",
    "        feature_values = np.expand_dims(x.iloc[:, feature_i], axis=1)\n",
    "        # 获取第i个特征的唯一取值\n",
    "        unique_values = np.unique(feature_values)\n",
    "        # 遍历取值并寻找最佳特征分裂阈值\n",
    "        for threshold in unique_values:\n",
    "            # 特征节点二叉分裂\n",
    "            xy1 = xy[xy[feature_list[feature_i]] >= threshold]\n",
    "            xy2 = xy[xy[feature_list[feature_i]] < threshold]\n",
    "            # xy1, xy2 = feature_split(xy, feature_i, threshold)\n",
    "            # 如果分裂后的子集大小都不为0\n",
    "            if len(xy1) > 0 and len(xy2) > 0:\n",
    "                # 获取两个子集的标签值\n",
    "                y1 = xy1.iloc[:, n_features:]\n",
    "                y2 = xy2.iloc[:, n_features:]\n",
    "                # 计算基尼不纯度\n",
    "                # impurity = impurity_calculation(y, y1, y2)\n",
    "                impurity = calculate_gini_impurity(y_true_pred, y1, y2)\n",
    "                # 获取最小基尼不纯度\n",
    "                # 最佳特征索引和分裂阈值\n",
    "                if impurity < init_gini_impurity:\n",
    "                    init_gini_impurity = impurity\n",
    "                    best_criteria = {\n",
    "                        \"feature_name\": feature_list[feature_i], \"threshold\": threshold}\n",
    "                    best_sets = {\n",
    "                        # \"leftx\": xy1.iloc[:, :n_features],\n",
    "                        # \"lefty\": xy1.iloc[:, n_features:],\n",
    "                        \"left_index\": xy1.index,\n",
    "                        # \"rightx\": xy2.iloc[:, :n_features],\n",
    "                        # \"righty\": xy2.iloc[:, n_features:],\n",
    "                        \"right_index\": xy2.index,\n",
    "                    }\n",
    "    gain_server = init_gini_impurity\n",
    "else:\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    gain_server = None\n",
    "print(gain_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encry_label\n",
    "from phe import paillier\n",
    "loss=LogisticLoss()\n",
    "y_true, y_pred = node_split(np.array(y_true_pred.to_list()))\n",
    "gradient_y = y_true * loss.gradient(y_true, y_pred)\n",
    "hess_y=loss.hess(y_true, y_pred)\n",
    "public_key, private_key = paillier.generate_paillier_keypair()\n",
    "      # 加密整个数据\n",
    "gradient_y_copy=[]\n",
    "hess_y_copy=[]\n",
    "for index, row in enumerate(gradient_y):\n",
    "    gradient_y_copy.append([public_key.encrypt(x) for x in row])\n",
    "for index, row in enumerate(hess_y):\n",
    "    hess_y_copy.append([public_key.encrypt(x) for x in row])\n",
    "gradient_y_copy = np.array(gradient_y_copy)\n",
    "hess_y_copy = np.array(hess_y_copy)\n",
    "gradient_y_copy = pd.DataFrame({\"gradient\":list(gradient_y_copy)}, index=y_true_pred.index)\n",
    "hess_y_copy = pd.DataFrame({\"hess\":list(hess_y_copy)}, index=y_true_pred.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guest  calu_encry_grdient_hess\n",
    "def gradient_hess_sum(gradient_y,hess_y,gradient_y1,hess_y1,gradient_y2,hess_y2):\n",
    "    gradient_y = np.array(gradient_y[\"gradient\"].to_list())\n",
    "    hess_y = np.array(hess_y[\"hess\"].to_list())\n",
    "    gradient_y1 = np.array(gradient_y1[\"gradient\"].to_list())\n",
    "    hess_y1 = np.array(hess_y1[\"hess\"].to_list())\n",
    "    gradient_y2 = np.array(gradient_y2[\"gradient\"].to_list())\n",
    "    hess_y2 = np.array(hess_y2[\"hess\"].to_list())\n",
    "    \n",
    "    \n",
    "    return gradient_y.sum(), hess_y.sum(), gradient_y1.sum(), hess_y1.sum(), gradient_y2.sum(), hess_y2.sum()\n",
    "feature_splite={}\n",
    "feature_splite_send = {}\n",
    "     # 合并输入和标签\n",
    "x_guest=x_guest.iloc[y_true_pred.index]\n",
    "# xy = pd.concat((x_guest, gradient_y_copy,hess_y_copy), axis=1)\n",
    "      # 获取样本数和特征数\n",
    "n_samples, n_features = x_guest.shape\n",
    "if n_samples >= min_samples_split and current_depth <= max_depth:\n",
    "    # 遍历计算每个特征的基尼不纯度\n",
    "    feature_list = x_guest.columns\n",
    "    for feature_i in range(n_features):\n",
    "        feature_splite[feature_i]=[]\n",
    "        feature_splite_send[feature_i]=[]\n",
    "        # 获取第i特征的所有取值\n",
    "        feature_values = np.expand_dims(x_guest.iloc[:, feature_i], axis=1)\n",
    "        # 获取第i个特征的唯一取值\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        # 遍历取值并寻找最佳特征分裂阈值\n",
    "        for threshold in unique_values:\n",
    "            # 特征节点二叉分裂\n",
    "            x1 = x_guest[x_guest[feature_list[feature_i]] >= threshold]\n",
    "            x2 = x_guest[x_guest[feature_list[feature_i]] < threshold]\n",
    "            # xy1, xy2 = feature_split(xy, feature_i, threshold)\n",
    "            # 如果分裂后的子集大小都不为0\n",
    "            if len(x1) > 0 and len(x2) > 0:\n",
    "                # 获取两个子集的标签值\n",
    "                # y1 = xy1.iloc[:, n_features:]\n",
    "                # y2 = xy2.iloc[:, n_features:]\n",
    "                gradient_y1=gradient_y_copy.iloc[x1.index]\n",
    "                gradient_y2 = gradient_y_copy.iloc[x2.index]\n",
    "                hess_y1 = hess_y_copy.iloc[x1.index]\n",
    "                hess_y2 = hess_y_copy.iloc[x2.index]\n",
    "                # 计算基尼不纯度\n",
    "                # impurity = self.impurity_calculation(y, y1, y2)\n",
    "                feature_splite_send[feature_i].append({threshold: gradient_hess_sum(\n",
    "                    gradient_y_copy, hess_y_copy, gradient_y1, hess_y1,gradient_y2,hess_y2)})\n",
    "                best_criteria = {\n",
    "                    \"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                best_sets = {\n",
    "                    # \"leftx\": xy1[:, :n_features],\n",
    "                    # \"lefty\": xy1[:, n_features:],\n",
    "                    \"left_index\":x1.index,\n",
    "                    \"right_index\":x2.index\n",
    "                    # \"rightx\": xy2[:, :n_features],\n",
    "                    # \"righty\": xy2[:, n_features:],\n",
    "                    \n",
    "                }\n",
    "                feature_splite[feature_i].append({threshold:[best_criteria, best_sets]})\n",
    "else:\n",
    "    feature_splite_send = None\n",
    "        # return {\"host\": feature_splite_send}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.552713678800501e-15\n"
     ]
    }
   ],
   "source": [
    "min_gain=999\n",
    "for key in feature_splite_send:\n",
    "    for value in feature_splite_send[key]:\n",
    "        # feature_i=key\n",
    "        # threshold=list(value.keys())[0]\n",
    "        gain_value=list(value.values())[0]\n",
    "        gain_real = 0.5 * \\\n",
    "            np.power(private_key.decrypt(\n",
    "                gain_value[0]), 2)/private_key.decrypt(gain_value[1])\n",
    "        gain_true = 0.5 * \\\n",
    "            np.power(private_key.decrypt(\n",
    "                gain_value[2]), 2)/private_key.decrypt(gain_value[3])\n",
    "        gain_false = 0.5 * \\\n",
    "            np.power(private_key.decrypt(\n",
    "                gain_value[4]), 2)/private_key.decrypt(gain_value[5])\n",
    "        gain_final=gain_true+gain_false-gain_real\n",
    "        if gain_final<min_gain:\n",
    "            min_gain=gain_final\n",
    "            feature_i=key\n",
    "            threshold=list(value.keys())[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class XGB:\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_score=0.5,\n",
    "                 max_depth=3,\n",
    "                 n_estimators=10,\n",
    "                 learning_rate=0.1,\n",
    "                 reg_lambda=1,\n",
    "                 gamma=0,\n",
    "                 min_child_sample=None,\n",
    "                 min_child_weight=1,\n",
    "                 objective='linear'):\n",
    "\n",
    "        self.base_score = base_score  # 最开始时给叶子节点权重所赋的值，默认0.5，迭代次数够多的话，结果对这个初值不敏感\n",
    "        self.max_depth = max_depth  # 最大数深度\n",
    "        self.n_estimators = n_estimators  # 树的个数\n",
    "        self.learning_rate = learning_rate  # 学习率，别和梯度下降里的学习率搞混了，这里是每棵树要乘以的权重系数\n",
    "        self.reg_lambda = reg_lambda  # L2正则项的权重系数\n",
    "        self.gamma = gamma  # 正则项中，叶子节点数T的权重系数\n",
    "        self.min_child_sample = min_child_sample  # 每个叶子节点的样本数（自己加的）\n",
    "        self.min_child_weight = min_child_weight  # 每个叶子节点的Hessian矩阵和，下面代码会细讲\n",
    "        self.objective = objective  # 目标函数，可选linear和logistic\n",
    "        self.tree_structure = {}  # 用一个字典来存储每一颗树的树结构\n",
    "\n",
    "    def xgb_cart_tree(self, X, w, m_dpth):\n",
    "        '''\n",
    "        递归的方式构造XGB中的Cart树\n",
    "        X：训练数据集\n",
    "        w：每个样本的权重值，递归赋值\n",
    "        m_dpth：树的深度\n",
    "        '''\n",
    "\n",
    "        #边界条件：递归到指定最大深度后，跳出\n",
    "        if m_dpth > self.max_depth:\n",
    "            return\n",
    "\n",
    "        best_var, best_cut = None, None\n",
    "        #这里增益的初值一定要设置为0，相当于对树做剪枝，即如果算出的增益小于0则不做分裂\n",
    "        max_gain = 0\n",
    "        G_left_best, G_right_best, H_left_best, H_right_best = 0, 0, 0, 0\n",
    "        #遍历每个变量的每个切点，寻找分裂增益gain最大的切点并记录下来\n",
    "        for item in [x for x in X.columns if x not in ['g', 'h', 'y']]:\n",
    "            for cut in list(set(X[item])):\n",
    "\n",
    "                #这里如果指定了min_child_sample则限制分裂后叶子节点的样本数都不能小于指定值\n",
    "                if self.min_child_sample:\n",
    "                    if (X.loc[X[item] < cut].shape[0] < self.min_child_sample)\\\n",
    "                            | (X.loc[X[item] >= cut].shape[0] < self.min_child_sample):\n",
    "                        continue\n",
    "\n",
    "                G_left = X.loc[X[item] < cut, 'g'].sum()\n",
    "                G_right = X.loc[X[item] >= cut, 'g'].sum()\n",
    "                H_left = X.loc[X[item] < cut, 'h'].sum()\n",
    "                H_right = X.loc[X[item] >= cut, 'h'].sum()\n",
    "\n",
    "                #min_child_weight在这里起作用，指的是每个叶子节点上的H，即目标函数二阶导的加和\n",
    "                #当目标函数为linear，即1/2*(y-y_hat)**2时，它的二阶导是1，那min_child_weight就等价于min_child_sample\n",
    "                #当目标函数为logistic，其二阶导为sigmoid(y_hat)*(1-sigmoid(y_hat))，可理解为叶子节点的纯度，更详尽的解释可参看：\n",
    "                #https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm#\n",
    "                if self.min_child_weight:\n",
    "                    if (H_left < self.min_child_weight) | (H_right < self.min_child_weight):\n",
    "                        continue\n",
    "\n",
    "                gain = G_left**2/(H_left + self.reg_lambda) + \\\n",
    "                    G_right**2/(H_right + self.reg_lambda) - \\\n",
    "                    (G_left + G_right)**2/(H_left + H_right + self.reg_lambda)\n",
    "                gain = gain/2 - self.gamma\n",
    "                if gain > max_gain:\n",
    "                    best_var, best_cut = item, cut\n",
    "                    max_gain = gain\n",
    "                    G_left_best, G_right_best, H_left_best, H_right_best = G_left, G_right, H_left, H_right\n",
    "\n",
    "        #如果遍历完找不到可分列的点，则返回None\n",
    "        if best_var is None:\n",
    "            return None\n",
    "\n",
    "        #给每个叶子节点上的样本分别赋上相应的权重值\n",
    "        id_left = X.loc[X[best_var] < best_cut].index.tolist()\n",
    "        w_left = - G_left_best / (H_left_best + self.reg_lambda)\n",
    "\n",
    "        id_right = X.loc[X[best_var] >= best_cut].index.tolist()\n",
    "        w_right = - G_right_best / (H_right_best + self.reg_lambda)\n",
    "\n",
    "        w[id_left] = w_left\n",
    "        w[id_right] = w_right\n",
    "\n",
    "        #用俄罗斯套娃式的json串把树的结构给存下来\n",
    "        tree_structure = {(best_var, best_cut): {}}\n",
    "        tree_structure[(best_var, best_cut)][('left', w_left)\n",
    "                                             ] = self.xgb_cart_tree(X.loc[id_left], w, m_dpth+1)\n",
    "        tree_structure[(best_var, best_cut)][('right', w_right)\n",
    "                                             ] = self.xgb_cart_tree(X.loc[id_right], w, m_dpth+1)\n",
    "\n",
    "        return tree_structure\n",
    "\n",
    "    def _grad(self, y_hat, Y):\n",
    "        '''\n",
    "        计算目标函数的一阶导\n",
    "        支持linear和logistic\n",
    "        '''\n",
    "\n",
    "        if self.objective == 'logistic':\n",
    "            y_hat = 1.0/(1.0+np.exp(-y_hat))\n",
    "            return y_hat - Y\n",
    "        elif self.objective == 'linear':\n",
    "            return y_hat - Y\n",
    "        else:\n",
    "            raise KeyError('objective must be linear or logistic!')\n",
    "\n",
    "    def _hess(self, y_hat, Y):\n",
    "        '''\n",
    "        计算目标函数的二阶导\n",
    "        支持linear和logistic\n",
    "        '''\n",
    "\n",
    "        if self.objective == 'logistic':\n",
    "            y_hat = 1.0/(1.0+np.exp(-y_hat))\n",
    "            return y_hat * (1.0 - y_hat)\n",
    "        elif self.objective == 'linear':\n",
    "            return np.array([1]*Y.shape[0])\n",
    "        else:\n",
    "            raise KeyError('objective must be linear or logistic!')\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, Y):\n",
    "        '''\n",
    "        根据训练数据集X和Y训练出树结构和权重\n",
    "        '''\n",
    "\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError('X and Y must have the same length!')\n",
    "\n",
    "        X = X.reset_index(drop='True')\n",
    "        Y = Y.values\n",
    "        #这里根据base_score参数设定权重初始值\n",
    "        y_hat = np.array([self.base_score]*Y.shape[0])\n",
    "        for t in range(self.n_estimators):\n",
    "\n",
    "            print('fitting tree {}...'.format(t+1))\n",
    "\n",
    "            X['g'] = self._grad(y_hat, Y)\n",
    "            X['h'] = self._hess(y_hat, Y)\n",
    "\n",
    "            f_t = pd.Series([0]*Y.shape[0])\n",
    "            self.tree_structure[t+1] = self.xgb_cart_tree(X, f_t, 1)\n",
    "\n",
    "            y_hat = y_hat + self.learning_rate * f_t\n",
    "\n",
    "            print('tree {} fit done!'.format(t+1))\n",
    "\n",
    "        print(self.tree_structure)\n",
    "\n",
    "    def _get_tree_node_w(self, X, tree, w):\n",
    "        '''\n",
    "        以递归的方法，把树结构解构出来，把权重值赋到w上面\n",
    "        '''\n",
    "\n",
    "        if not tree is None:\n",
    "            k = list(tree.keys())[0]\n",
    "            var, cut = k[0], k[1]\n",
    "            X_left = X.loc[X[var] < cut]\n",
    "            id_left = X_left.index.tolist()\n",
    "            X_right = X.loc[X[var] >= cut]\n",
    "            id_right = X_right.index.tolist()\n",
    "            for kk in tree[k].keys():\n",
    "                if kk[0] == 'left':\n",
    "                    tree_left = tree[k][kk]\n",
    "                    w[id_left] = kk[1]\n",
    "                elif kk[0] == 'right':\n",
    "                    tree_right = tree[k][kk]\n",
    "                    w[id_right] = kk[1]\n",
    "\n",
    "            self._get_tree_node_w(X_left, tree_left, w)\n",
    "            self._get_tree_node_w(X_right, tree_right, w)\n",
    "\n",
    "    def predict_raw(self, X: pd.DataFrame):\n",
    "        '''\n",
    "        根据训练结果预测\n",
    "        返回原始预测值\n",
    "        '''\n",
    "\n",
    "        X = X.reset_index(drop='True')\n",
    "        Y = pd.Series([self.base_score]*X.shape[0])\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            tree = self.tree_structure[t+1]\n",
    "            y_t = pd.Series([0]*X.shape[0])\n",
    "            self._get_tree_node_w(X, tree, y_t)\n",
    "            Y = Y + self.learning_rate * y_t\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def predict_prob(self, X: pd.DataFrame):\n",
    "        '''\n",
    "        当指定objective为logistic时，输出概率要做一个logistic转换\n",
    "        '''\n",
    "\n",
    "        Y = self.predict_raw(X)\n",
    "        def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "        Y = Y.apply(sigmoid)\n",
    "\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data1/jhjiang/iflearner/iflearner/business/hetero/model/Xgboost/test.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.1.86.60/data1/jhjiang/iflearner/iflearner/business/hetero/model/Xgboost/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(dict_threshold\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0.8\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.1.86.60/data1/jhjiang/iflearner/iflearner/business/hetero/model/Xgboost/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     leftx \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dict_threshold\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mleftx\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.1.86.60/data1/jhjiang/iflearner/iflearner/business/hetero/model/Xgboost/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     lefty \u001b[39m=\u001b[39m dict_threshold\u001b[39m.\u001b[39mvalues()[\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlefty\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.1.86.60/data1/jhjiang/iflearner/iflearner/business/hetero/model/Xgboost/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     rightx \u001b[39m=\u001b[39m dict_threshold\u001b[39m.\u001b[39mvalues()[\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrightx\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.1.86.60/data1/jhjiang/iflearner/iflearner/business/hetero/model/Xgboost/test.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     righty \u001b[39m=\u001b[39m dict_threshold\u001b[39m.\u001b[39mvalues()[\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrighty\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "feature_splite={0:[]}\n",
    "best_criteria = {\"feature_name\": \"a\", \"threshold\": 0.8}\n",
    "best_sets = {\"leftx\": 1,\n",
    "            \"lefty\": 2,\n",
    "            \"rightx\": 3,\n",
    "            \"righty\":4,\n",
    "                        }\n",
    "feature_splite[0].append({0.8: [best_criteria, best_sets]})\n",
    "for dict_threshold in feature_splite[0]:\n",
    "    if list(dict_threshold.keys())[0] == 0.8:\n",
    "        leftx = list(dict_threshold.values())[0][1][\"leftx\"]\n",
    "        lefty = dict_threshold.values()[1][\"lefty\"]\n",
    "        rightx = dict_threshold.values()[1][\"rightx\"]\n",
    "        righty = dict_threshold.values()[1][\"righty\"]\n",
    "        feature_name = dict_threshold.values()[0][\"feature_name\"]\n",
    "        threshold = dict_threshold.values()[0][\"threshold\"]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('iflearner')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21ed6e169b0364d8fca31f96b8a2af9a5d34cf3cbdbfa3cfa47eb5ed1786d031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
